{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-XU3_aRitCbl","outputId":"5549382d-4efa-4660-84cf-508384bcfe74","executionInfo":{"status":"ok","timestamp":1695479534165,"user_tz":-330,"elapsed":5566,"user":{"displayName":"Krishna Swamy","userId":"04249444362890486958"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting openai\n","  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n","Installing collected packages: openai\n","Successfully installed openai-0.28.0\n"]}],"source":["!pip install openai"]},{"cell_type":"code","source":["!pip install --upgrade pymupdf"],"metadata":{"id":"ZSfTikpK0Bu5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fbf4ad9d-cd4d-455b-a209-89be0fe4efcc","executionInfo":{"status":"ok","timestamp":1695479577356,"user_tz":-330,"elapsed":6456,"user":{"displayName":"Krishna Swamy","userId":"04249444362890486958"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pymupdf\n","  Downloading PyMuPDF-1.23.3-cp310-none-manylinux2014_x86_64.whl (4.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting PyMuPDFb==1.23.3 (from pymupdf)\n","  Downloading PyMuPDFb-1.23.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n","Successfully installed PyMuPDFb-1.23.3 pymupdf-1.23.3\n"]}]},{"cell_type":"code","source":["import os\n","import openai\n","\n","openai.api_key =  ''#generate your api_key"],"metadata":{"id":"XajkO-wEtnXp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import fitz"],"metadata":{"id":"srz45bfF0DJ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oqq3Zm2_194F","executionInfo":{"status":"ok","timestamp":1695479647387,"user_tz":-330,"elapsed":23910,"user":{"displayName":"Krishna Swamy","userId":"04249444362890486958"}},"outputId":"98cdad2e-cab1-4760-bc25-ebf761196e42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["doc = fitz.open('/content/drive/MyDrive/NLP/NLP/OpenAI/attention.pdf')"],"metadata":{"id":"vfWpy7_QVC8t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary_list =[]\n","for page in doc:\n","  text = page.get_text(\"text\")\n","  #print(text)\n","  prompt= text + \"\\n Tl;dr:\"\n","  response = openai.Completion.create(\n","  model=\"text-davinci-003\",\n","  prompt=prompt,\n","  temperature=0.7,\n","  max_tokens=120,\n","  top_p=0.9,\n","  frequency_penalty=0.0,\n","  presence_penalty=1\n","  )\n","  summary_list.append(response[\"choices\"][0][\"text\"])\n"],"metadata":{"id":"Ni19lQsoVJKP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary_text=' '.join(summary_list)\n","print(summary_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bUjynEGNXcbT","outputId":"d11b4b11-24f0-4c4b-8b56-e7d01badd2eb","executionInfo":{"status":"ok","timestamp":1695479966923,"user_tz":-330,"elapsed":568,"user":{"displayName":"Krishna Swamy","userId":"04249444362890486958"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, and Łukasz Kaiser propose a new network architecture, the Transformer, which is based solely on attention mechanisms and dispenses with recurrence and convolutions entirely. Experiments show that this model outperforms existing models in machine translation tasks while being more parallelizable and requiring less time to train.  This paper presents the Transformer, a model architecture which uses self-attention mechanisms to draw global dependencies between input and output. This allows for more parallelization than traditional recurrent models, resulting in improved computational efficiency and translation quality. \n","The Transformer model consists of an encoder and a decoder stack composed of 6 identical layers. Each layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network, with residual connections around each sub-layer and layer normalization. The decoder also includes a third sub-layer that performs multi-head attention over the output of the encoder stack. Attention is a function that maps a query and a set of key-value pairs to an output vector, where the weight assigned to each value is computed by \n","Scaled Dot-Product Attention is an attention mechanism which computes the dot products of queries and keys, divides each by √dk, and applies a softmax function to obtain the weights on the values. Multi-Head Attention is a variant of this approach which linearly projects the queries, keys and values h times with different, learned linear projections and performs the attention function in parallel. \n","Multi-head attention allows a model to attend to different representation subspaces at different positions. It consists of projections from the query, key and value matrices with h parallel attention layers. This is used in the encoder-decoder attention layer, the encoder's self-attention layer and the decoder's self-attention layer. In addition to these, each layer contains a position-wise feed-forward network consisting of two linear transformations with a ReLU activation, embeddings and softmax, and a positional encoding. \n","Self-attention layers have a constant number of sequential operations and lower computational complexity than recurrent layers, making them faster and more efficient for tasks involving long sequences. The maximum path length between any two input and output positions in a network composed of self-attention layers is also shorter than that of recurrent layers, allowing for easier learning of long-range dependencies.  We trained a model on the WMT 2014 English-German dataset with 8 NVIDIA P100 GPUs. The Adam optimizer was used with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We employed residual dropout and label smoothing regularization. The base model was trained for 100,000 steps and the big model for 300,000 steps.  The Transformer model achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. We use label smoothing, beam search with a beam size of 4, and length penalty α = 0.6 during inference. We found that varying the number of attention heads and the attention key and value dimensions kept the amount of computation constant.  We compare variations on the Transformer architecture and find that reducing the attention key size hurts model quality, bigger models are better, and dropout is very helpful in avoiding overfitting. We further observe that replacing our sinusoidal positional encoding with learned positional embeddings results in nearly identical results to the base model. We also evaluate if the Transformer can generalize to English constituency parsing and find that it achieves a 92.7 F1 score when semi-supervised.  This paper presents the Transformer, a sequence transduction model based entirely on attention. It can be trained significantly faster than architectures based on recurrent or convolutional layers and achieves state-of-the-art results for translation tasks. It is also applied to other tasks such as language modeling and image recognition. We plan to extend the Transformer to other problems involving input and output modalities other than text and investigate local, restricted attention mechanisms.  This paper surveys the recent advances in recurrent neural networks (RNNs) for sequence modeling tasks such as language modeling, machine translation, and parsing. It discusses various types of RNNs such as long short-term memory (LSTM), gated recurrent units (GRUs), and attention-based models, and describes their advantages and disadvantages. The paper also highlights some recent developments in the area, such as self-attention networks, factorization tricks, and multi-task learning. \n","Research has shown that neural networks can be used for many natural language processing tasks, such as summarization, machine translation, and parsing. Additionally, techniques such as dropout and subword units have been developed to improve the accuracy of these models. Furthermore, recent advances in deep learning architectures, such as the Transformer and Mixture-of-Experts layers, are being applied to further boost performance.  The attention mechanism in layer 5 of the encoder self-attention follows long-distance dependencies, with many of the heads attending to a distant dependency of the verb 'making', completing the phrase 'making...more difficult'.  Two attention heads in layer 5 of the neural network are apparently involved in anaphora resolution, with the attentions for the word \"its\" being very sharp.  Attention heads in the encoder self-attention layer of a deep learning model have learned to perform different tasks, depending on the structure of the sentence. Examples include heads that focus on the syntax of the sentence and heads that focus on semantic meaning.\n"]}]},{"cell_type":"code","source":["prompt= summary_text + \"\\n Tl;dr:\"\n","response = openai.Completion.create(\n","model=\"text-davinci-003\",\n","prompt=prompt,\n","temperature=0.7,\n","max_tokens=400,\n","top_p=0.9,\n","frequency_penalty=0.0,\n","presence_penalty=1\n",")\n","print(response[\"choices\"][0][\"text\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lrN3AgYzZHzd","outputId":"bb7749b5-fd4d-42ba-dce4-0ff4e2453752","executionInfo":{"status":"ok","timestamp":1695479985357,"user_tz":-330,"elapsed":2089,"user":{"displayName":"Krishna Swamy","userId":"04249444362890486958"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" This paper presents the Transformer, a sequence transduction model based on attention mechanisms. Experiments show that this model outperforms existing models in machine translation tasks while being more parallelizable and requiring less time to train. It is also applicable to other tasks such as language modeling and image recognition. Improvements over traditional recurrent models include increased computational efficiency, improved translation quality, and better learning of long-range dependencies.\n"]}]},{"cell_type":"code","source":["print(response)"],"metadata":{"id":"Np0zcc0IajDP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary_list =[]\n","for page in doc:\n","  text = page.get_text(\"text\")\n","  #print(text)\n","  prompt= \"Summarize this for a second-grade student: \" +text\n","  response = openai.Completion.create(\n","  model=\"text-davinci-003\",\n","  prompt=prompt,\n","  temperature=0.7,\n","  max_tokens=120,\n","  top_p=0.9,\n","  frequency_penalty=0.0,\n","  presence_penalty=1\n","  )\n","  summary_list.append(response[\"choices\"][0][\"text\"])\n"],"metadata":{"id":"r4WmT6W5ePIk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary_text= ' '.join(summary_list)\n","print(summary_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NUYSaUW6f8-H","outputId":"caf5973d-0aaf-490c-cec8-779d1da3cd00","executionInfo":{"status":"ok","timestamp":1695480107839,"user_tz":-330,"elapsed":18,"user":{"displayName":"Krishna Swamy","userId":"04249444362890486958"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","This paper talks about a new type of machine learning model called the Transformer. It is based on attention mechanisms and does not use recurrence or convolutions. This model is better at doing certain tasks like machine translation (changing one language into another) and English constituency parsing (identifying the structure of sentences). It can do these tasks faster and more accurately than other models. A second-grade student can understand that scientists are trying to make computers better at understanding language by using models and architectures. They are making the process faster and more efficient so that computers can do tasks like translating and summarizing quickly. \n","The Transformer is a model architecture that uses two stacks of layers, an encoder and decoder. It uses something called self-attention, which helps to determine how the pieces of information fit together. The decoder also uses multi-head attention over the output from the encoder stack. This helps the model understand which pieces of information are important. \n","Multi-Head Attention is a way of combining several attention layers together to process information. It takes queries, keys, and values and projects them multiple times with different, learned linear projections. The attention layers are then run in parallel and the results are combined and projected again for the final output. tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the\n","bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n","as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n","learned and fixed [21], but we use sine and cosine functions of different frequencies:\n","PE(pos, 2i) = sin(pos/100002i/dmodel)\n"," This section compares different layers for mapping one variable-length sequence of symbols to another sequence. Self-attention layers have a constant number of sequential operations, while recurrent layers require more. Self-attention layers can also learn long-range dependencies faster than other types of layers. Conclusion\n","We presented a new model based on the transformer architecture that uses self-attention and separable convolutions for efficient training and inference. Our results show that it achieves state-of-the-art performance on multiple machine translation tasks while being more computationally efficient than existing models. We hope that our work will encourage future research into the combination of self-attention and convolutional layers.\n","\n","We have created a new type of computer model that helps computers understand language better. It combines two different types of technology to make it more efficient and give better results. We tested it on A second-grade student can understand that the Transformer model is better than other state-of-the-art models at translating English into German and French, and it is faster to train. It also uses less training cost and has a higher BLEU score. \n","The Transformer architecture can be changed in different ways to make it better. Changing the attention key size, making the model bigger, and adding dropout can all help improve the model's performance. We also tried replacing a sinusoidal positional encoding with learned positional embeddings and found that it worked just as well. We also tested this architecture on English constituency parsing and it did well. \n","The Transformer is a new type of model that uses attention instead of recurrent layers to do tasks like translating words from one language to another. It can be trained faster and better than other models, and it outperformed all previously reported models on two translation tasks. We made the code available so others can use it for their own tasks. \n","Four scientists, Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio studied how gated neural networks work. They looked at ways to use them for understanding sequences of things. Other scientists also studied this topic and found ways to make the networks better. They all tried to find ways to help computers understand language better. \n","27 people wrote papers about improving the way computers understand language. These papers have different ideas like using tree annotation, subword units, output embedding, and mixture-of-experts layers. Many governments in America have made it harder to register or vote since 2009. The law is not perfect, but it should be applied fairly. This is something that we need to work on. We need to make sure that the law is applied fairly and justly, even if it is not perfect.\n"]}]},{"cell_type":"code","source":["prompt= \"Summarize this for a second-grade student: \"+summary_text\n","response = openai.Completion.create(\n","model=\"text-davinci-003\",\n","prompt=prompt,\n","temperature=0.7,\n","max_tokens=400,\n","top_p=0.9,\n","frequency_penalty=0.0,\n","presence_penalty=1\n",")\n","print(response[\"choices\"][0][\"text\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nScXhI01gDiK","outputId":"1fc50592-5943-4abe-b783-737f4f9475cd","executionInfo":{"status":"ok","timestamp":1695480117368,"user_tz":-330,"elapsed":1476,"user":{"displayName":"Krishna Swamy","userId":"04249444362890486958"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"T3hZO60FgU_3"},"execution_count":null,"outputs":[]}]}