{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO2E0CB701wkIU5pqejqVF/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BY6LdjnET00U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695477616882,"user_tz":-330,"elapsed":7466,"user":{"displayName":"Krishna Swamy","userId":"04249444362890486958"}},"outputId":"760b6291-acb6-4474-ced5-8c91e7a470f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/76.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["pip install openai -q"]},{"cell_type":"code","source":["\"\"\"\n","TABLE OF CONTENTS\n","ChatGPT\n","Chat vs Completions\n","ChatGPT API\n","Building a Conversational AI Chatbot\n","Fine-tuning with gpt-3.5-turbo\n","\n","\"\"\""],"metadata":{"id":"HuuYVrNPUE7C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import openai\n","openai.api_key = '' #generate your api_key"],"metadata":{"id":"ERm0JIakT2jc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","ChatGPT\n","\n","ChatGPT is a powerful API allowing developers to quickly create conversational AI applications.\n","With ChatGPT, developers can build chatbots, virtual assistants, and other conversational AI applications\n","that can engage with users in a natural, human-like manner.\n","\n","\"\"\""],"metadata":{"id":"WnH2ipB4T6iv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","\n","Chat vs Completions\n","ChatGPT's gpt-3.5-turbo model provides an affordable alternative to text-davinci-003, while maintaining a similar level of performance.\n","This makes it the preferred choice for most use cases, as it offers significant cost savings without sacrificing quality.\n","\n","Transitioning to gpt-3.5-turbo is often a straightforward process for developers, requiring only minor adjustments to prompts and retesting.\n"," For instance, if you were using a completions prompt to translate English to French,\n"," you could easily switch to the gpt-3.5-turbo model by rewriting your prompt accordingly.\n","\n","\"\"\"\n"],"metadata":{"id":"XyOW_oPnUVxV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","\n","Building a Conversational AI Chatbot\n","\n","you'll need to define your conversation.\n","\n","Conversations are defined as an array of message objects, where each object has a role (either \"system\", \"user\", or \"assistant\")\n","\n","and content (the content of the message). Conversations can be as short as one message or as long as you like.\n","\n","Typically, a conversation is formatted with a system message first, followed by alternating user and assistant messages."],"metadata":{"id":"_A1GzEYNUyeE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","\n","messages = [\n","    {\"role\": \"system\", \"content\": \"You are a helpful AI Tutor.\"},\n","    {\"role\": \"user\", \"content\": \"I am Abhiprakash, I want to learn AI\"},\n","    {\"role\": \"assistant\", \"content\": \"Hello, Abhiprakash. That's awesome! What do you want to know about AI?\"},\n","    {\"role\": \"user\", \"content\": \"What is NLP?\"}\n","]\n","\n","\"\"\""],"metadata":{"id":"xo2T9sSPVIk9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","\n","To get a response from the ChatGPT API, you can use the openai.ChatCompletion.create() method.\n","This method takes two arguments: the name of the model to use and the conversation that you defined earlier.\n","\n","\"\"\""],"metadata":{"id":"7Mop8zNjViec"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response = openai.ChatCompletion.create(\n","  model=\"gpt-3.5-turbo\",\n","  messages = [\n","    {\"role\": \"system\", \"content\": \"You are a helpful AI Tutor.\"},\n","    {\"role\": \"user\", \"content\": \"I am Abhiprakash, I want to learn AI\"},\n","    {\"role\": \"assistant\", \"content\": \"Hello, Abhiprakash. That's awesome! What do you want to know about AI?\"},\n","    {\"role\": \"user\", \"content\": \"What is Transformers?\"}\n","]\n",")"],"metadata":{"id":"QFUf67DJVtnJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","\n","The openai.ChatCompletion.create() method returns a response object that contains the AI's response.\n","You can get the AI's response by accessing the choices list and then the message object.\n","\n","\"\"\""],"metadata":{"id":"ldLWOoLPWEl_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u610x6lkWXL-","executionInfo":{"status":"ok","timestamp":1695478301627,"user_tz":-330,"elapsed":824,"user":{"displayName":"Krishna Swamy","userId":"04249444362890486958"}},"outputId":"5dbbbaab-360a-4b9e-fda2-ff536f645cca"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<OpenAIObject chat.completion id=chatcmpl-81xdlBdFVg1d4H1CvLpxKi8xMu075 at 0x79650f994130> JSON: {\n","  \"id\": \"chatcmpl-81xdlBdFVg1d4H1CvLpxKi8xMu075\",\n","  \"object\": \"chat.completion\",\n","  \"created\": 1695478277,\n","  \"model\": \"gpt-3.5-turbo-0613\",\n","  \"choices\": [\n","    {\n","      \"index\": 0,\n","      \"message\": {\n","        \"role\": \"assistant\",\n","        \"content\": \"Transformers refer to a machine learning model architecture that has gained significant popularity and success in various natural language processing (NLP) tasks. It was introduced in 2017 by Vaswani et al. and has since become the go-to model for tasks like machine translation, question answering, text generation, and more.\\n\\nThe Transformers model is based on the concept of attention, which allows the model to focus on specific words or tokens within the input text, rather than processing the entire text simultaneously. This attention mechanism enables the model to understand the contextual relationships between words and generate more accurate outputs.\\n\\nIn Transformers, the input text is divided into a sequence of tokens, and each token attends to other tokens in the sequence. This self-attention mechanism allows the model to capture dependencies and patterns at different positions in the text, making it highly effective in capturing long-range dependencies.\\n\\nThe most famous implementation of the Transformer model is the \\\"BERT\\\" (Bidirectional Encoder Representations from Transformers), which has demonstrated outstanding performance in various NLP tasks.\\n\\nTransformers have revolutionized the field of NLP by providing powerful language representation capabilities and yielding state-of-the-art performance on various benchmark datasets.\"\n","      },\n","      \"finish_reason\": \"stop\"\n","    }\n","  ],\n","  \"usage\": {\n","    \"prompt_tokens\": 62,\n","    \"completion_tokens\": 234,\n","    \"total_tokens\": 296\n","  }\n","}"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["response['choices'][0]['message']['content']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"id":"FVdfo1zPWYJB","executionInfo":{"status":"ok","timestamp":1695478382833,"user_tz":-330,"elapsed":813,"user":{"displayName":"Krishna Swamy","userId":"04249444362890486958"}},"outputId":"77979556-a0e5-4f05-b802-fc4550a03301"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Transformers refer to a machine learning model architecture that has gained significant popularity and success in various natural language processing (NLP) tasks. It was introduced in 2017 by Vaswani et al. and has since become the go-to model for tasks like machine translation, question answering, text generation, and more.\\n\\nThe Transformers model is based on the concept of attention, which allows the model to focus on specific words or tokens within the input text, rather than processing the entire text simultaneously. This attention mechanism enables the model to understand the contextual relationships between words and generate more accurate outputs.\\n\\nIn Transformers, the input text is divided into a sequence of tokens, and each token attends to other tokens in the sequence. This self-attention mechanism allows the model to capture dependencies and patterns at different positions in the text, making it highly effective in capturing long-range dependencies.\\n\\nThe most famous implementation of the Transformer model is the \"BERT\" (Bidirectional Encoder Representations from Transformers), which has demonstrated outstanding performance in various NLP tasks.\\n\\nTransformers have revolutionized the field of NLP by providing powerful language representation capabilities and yielding state-of-the-art performance on various benchmark datasets.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["#With these basic steps, you can build a simple conversational AI application"],"metadata":{"id":"jWBc8lMZWemj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def update_chat(messages, role, content):\n","  messages.append({\"role\": role, \"content\": content})\n","  return messages"],"metadata":{"id":"CFoZzuHGWpgG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","\n","The update_chat() function is a helper function that takes in three arguments: messages, role, and content.\n","It then appends a new message to the messages list, which represents the conversation history.\n","The new message is created as a dictionary with two keys: role and content.\n","The role key represents the role of the message sender, which can be either \"user\", \"assistant\", or \"system\".\n","The content key represents the content of the message, which can be any text string.\n","\n","After adding the new message to the messages list, the function returns the updated messages list.\n","This function is useful in building a conversation history that can be used as input to the ChatGPT API for generating responses.\n","The conversation history allows the API to provide more context-aware responses, which can lead to a more natural and engaging conversation experience.\n","\n","\"\"\""],"metadata":{"id":"aJOPDlOIW4j4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_chatgpt_response(messages):\n","  response = openai.ChatCompletion.create(\n","  model=\"gpt-3.5-turbo\",\n","  messages=messages\n",")\n","  return  response['choices'][0]['message']['content']\n"],"metadata":{"id":"lRf2kzuNXQqf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","\n","The get_chatgpt_response function takes a list of messages as an input and returns the generated response from the OpenAI GPT-3 model.\n","\n","The function first calls the openai.ChatCompletion.create() method to generate a response. This method takes two arguments: the name of the model to use (in this case, the \"davinci\" model) and the conversation history, represented as a list of messages.\n","\n","After the response is generated, the function extracts the generated text from the choices attribute of the response. The choices attribute is a list of objects that represent different completions of the prompt, and the [0] index selects the top-scoring completion.\n","\n","Overall, this function provides a convenient way to generate responses from the OpenAI GPT-3 model given a conversation history\n","\n","\"\"\""],"metadata":{"id":"XQrx9hSLXXBE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pprint\n","\n","messages=[\n","      {\"role\": \"system\", \"content\": \"You are a helpful AI Tutor.\"},\n","    {\"role\": \"user\", \"content\": \"I am Abhiprakash, I want to learn AI\"},\n","    {\"role\": \"assistant\", \"content\": \"Hello, Abhiprakash. That's awesome! What do you want to know about AI?\"},\n","  ]\n","\n","while True:\n","  pprint.pprint(messages)\n","  user_input = input()\n","  messages = update_chat(messages, \"user\", user_input)\n","  model_response = get_chatgpt_response(messages)\n","  messages = update_chat(messages, \"assistant\", model_response)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"iAps8arpYJA5","outputId":"019771b2-8337-4044-e0fc-5b4d4a431e82","executionInfo":{"status":"error","timestamp":1695479163755,"user_tz":-330,"elapsed":385900,"user":{"displayName":"Krishna Swamy","userId":"04249444362890486958"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'content': 'You are a helpful AI Tutor.', 'role': 'system'},\n"," {'content': 'I am Abhiprakash, I want to learn AI', 'role': 'user'},\n"," {'content': \"Hello, Abhiprakash. That's awesome! What do you want to know \"\n","             'about AI?',\n","  'role': 'assistant'}]\n","what is transformer?\n","[{'content': 'You are a helpful AI Tutor.', 'role': 'system'},\n"," {'content': 'I am Abhiprakash, I want to learn AI', 'role': 'user'},\n"," {'content': \"Hello, Abhiprakash. That's awesome! What do you want to know \"\n","             'about AI?',\n","  'role': 'assistant'},\n"," {'content': 'what is transformer?', 'role': 'user'},\n"," {'content': 'The Transformer is a type of neural network architecture '\n","             'introduced in a paper called \"Attention is All You Need\" by '\n","             'Vaswani et al. in 2017. It revolutionized natural language '\n","             'processing (NLP) tasks and has been widely adopted in different '\n","             'domains, including machine translation, text generation, and '\n","             'speech recognition.\\n'\n","             '\\n'\n","             'The Transformer architecture uses a unique attention mechanism '\n","             'that allows it to look at different parts of the input sequence '\n","             'simultaneously. The attention mechanism enables the model to '\n","             'understand the context and relationships between different words '\n","             'or tokens in the sequence.\\n'\n","             '\\n'\n","             'Traditionally, recurrent neural networks (RNNs) were used for '\n","             'sequence generation tasks. However, Transformers have numerous '\n","             'advantages over RNNs. They can process sequences in parallel, '\n","             'making them more efficient for training on large datasets. '\n","             'Additionally, they can capture long-range dependencies more '\n","             'effectively due to their attention mechanism.\\n'\n","             '\\n'\n","             'The core components of a Transformer architecture include:\\n'\n","             '\\n'\n","             '1. Encoder: This component takes the input sequence and '\n","             'transforms it into a sequence of hidden states, capturing the '\n","             'contextual information.\\n'\n","             '\\n'\n","             '2. Decoder: It generates the output sequence based on the '\n","             'encoded representations and attended context from the encoder.\\n'\n","             '\\n'\n","             '3. Attention mechanism: This is a crucial part of the '\n","             'Transformer. It computes the weighted sum of the input sequence '\n","             'elements, giving importance to specific words based on their '\n","             'relevance to the current context.\\n'\n","             '\\n'\n","             'Transformers have significantly advanced the field of NLP and '\n","             'have been used in numerous state-of-the-art models, such as '\n","             \"OpenAI's GPT, Google's BERT, and many other language models. \"\n","             'They enable models to understand and generate human-like '\n","             'language by capturing extensive context and relationships.',\n","  'role': 'assistant'}]\n","what are applications of transformers?\n","[{'content': 'You are a helpful AI Tutor.', 'role': 'system'},\n"," {'content': 'I am Abhiprakash, I want to learn AI', 'role': 'user'},\n"," {'content': \"Hello, Abhiprakash. That's awesome! What do you want to know \"\n","             'about AI?',\n","  'role': 'assistant'},\n"," {'content': 'what is transformer?', 'role': 'user'},\n"," {'content': 'The Transformer is a type of neural network architecture '\n","             'introduced in a paper called \"Attention is All You Need\" by '\n","             'Vaswani et al. in 2017. It revolutionized natural language '\n","             'processing (NLP) tasks and has been widely adopted in different '\n","             'domains, including machine translation, text generation, and '\n","             'speech recognition.\\n'\n","             '\\n'\n","             'The Transformer architecture uses a unique attention mechanism '\n","             'that allows it to look at different parts of the input sequence '\n","             'simultaneously. The attention mechanism enables the model to '\n","             'understand the context and relationships between different words '\n","             'or tokens in the sequence.\\n'\n","             '\\n'\n","             'Traditionally, recurrent neural networks (RNNs) were used for '\n","             'sequence generation tasks. However, Transformers have numerous '\n","             'advantages over RNNs. They can process sequences in parallel, '\n","             'making them more efficient for training on large datasets. '\n","             'Additionally, they can capture long-range dependencies more '\n","             'effectively due to their attention mechanism.\\n'\n","             '\\n'\n","             'The core components of a Transformer architecture include:\\n'\n","             '\\n'\n","             '1. Encoder: This component takes the input sequence and '\n","             'transforms it into a sequence of hidden states, capturing the '\n","             'contextual information.\\n'\n","             '\\n'\n","             '2. Decoder: It generates the output sequence based on the '\n","             'encoded representations and attended context from the encoder.\\n'\n","             '\\n'\n","             '3. Attention mechanism: This is a crucial part of the '\n","             'Transformer. It computes the weighted sum of the input sequence '\n","             'elements, giving importance to specific words based on their '\n","             'relevance to the current context.\\n'\n","             '\\n'\n","             'Transformers have significantly advanced the field of NLP and '\n","             'have been used in numerous state-of-the-art models, such as '\n","             \"OpenAI's GPT, Google's BERT, and many other language models. \"\n","             'They enable models to understand and generate human-like '\n","             'language by capturing extensive context and relationships.',\n","  'role': 'assistant'},\n"," {'content': 'what are applications of transformers?', 'role': 'user'},\n"," {'content': 'Transformers have found extensive applications across various '\n","             'domains. Some of the notable applications of Transformers '\n","             'include:\\n'\n","             '\\n'\n","             '1. Machine Translation: Transformers have greatly improved '\n","             'machine translation. Models like Google\\'s \"Transformer\" and '\n","             'Facebook\\'s \"Fairseq\" have achieved high-quality translations by '\n","             'leveraging the attention mechanism to capture contextual '\n","             'information.\\n'\n","             '\\n'\n","             '2. Natural Language Processing (NLP): Transformers have been '\n","             'significant in advancing NLP tasks, such as text classification, '\n","             'sentiment analysis, named entity recognition, question '\n","             'answering, and summarization. Large-scale pre-trained '\n","             'transformer models like BERT (Bidirectional Encoder '\n","             'Representations from Transformers) and GPT (Generative '\n","             'Pre-trained Transformer) have achieved state-of-the-art '\n","             'performance in a wide range of NLP tasks.\\n'\n","             '\\n'\n","             '3. Speech Recognition: Transformers have been applied to speech '\n","             'recognition tasks, improving accuracy and performance. By '\n","             'considering the contextual dependencies in speech sequences, '\n","             'Transformers have shown promising results in transcribing audio '\n","             'to text.\\n'\n","             '\\n'\n","             '4. Image Recognition: Transformers have also been used in '\n","             'computer vision tasks, including image classification, object '\n","             'detection, and image generation. Vision Transformer (ViT) is an '\n","             'application example where transformers process image patches and '\n","             'outperform traditional convolutional neural networks (CNNs) in '\n","             'certain cases.\\n'\n","             '\\n'\n","             '5. Reinforcement Learning: Transformers have been combined with '\n","             'reinforcement learning algorithms to enable better '\n","             'decision-making in dynamic environments. These models can learn '\n","             'to perform complex tasks by interacting with an environment and '\n","             'receiving feedback.\\n'\n","             '\\n'\n","             '6. Recommendation Systems: Transformers have been applied to '\n","             'recommender systems, enhancing personalization and understanding '\n","             'user preferences. They can model sequential dependencies between '\n","             'user actions and capture complex patterns in user behavior.\\n'\n","             '\\n'\n","             '7. Generative Models: Transformers have been used to generate '\n","             'text and other content, including poetry, stories, and code. '\n","             'With their capacity to model long-range dependencies and '\n","             'contextual information, transformers can generate coherent and '\n","             'contextually relevant outputs.\\n'\n","             '\\n'\n","             'The applications of transformers are ever-expanding as '\n","             'researchers continue to explore and develop new architectures '\n","             'and techniques that leverage the power of transformers in '\n","             'various domains.',\n","  'role': 'assistant'}]\n","what is my name?\n","[{'content': 'You are a helpful AI Tutor.', 'role': 'system'},\n"," {'content': 'I am Abhiprakash, I want to learn AI', 'role': 'user'},\n"," {'content': \"Hello, Abhiprakash. That's awesome! What do you want to know \"\n","             'about AI?',\n","  'role': 'assistant'},\n"," {'content': 'what is transformer?', 'role': 'user'},\n"," {'content': 'The Transformer is a type of neural network architecture '\n","             'introduced in a paper called \"Attention is All You Need\" by '\n","             'Vaswani et al. in 2017. It revolutionized natural language '\n","             'processing (NLP) tasks and has been widely adopted in different '\n","             'domains, including machine translation, text generation, and '\n","             'speech recognition.\\n'\n","             '\\n'\n","             'The Transformer architecture uses a unique attention mechanism '\n","             'that allows it to look at different parts of the input sequence '\n","             'simultaneously. The attention mechanism enables the model to '\n","             'understand the context and relationships between different words '\n","             'or tokens in the sequence.\\n'\n","             '\\n'\n","             'Traditionally, recurrent neural networks (RNNs) were used for '\n","             'sequence generation tasks. However, Transformers have numerous '\n","             'advantages over RNNs. They can process sequences in parallel, '\n","             'making them more efficient for training on large datasets. '\n","             'Additionally, they can capture long-range dependencies more '\n","             'effectively due to their attention mechanism.\\n'\n","             '\\n'\n","             'The core components of a Transformer architecture include:\\n'\n","             '\\n'\n","             '1. Encoder: This component takes the input sequence and '\n","             'transforms it into a sequence of hidden states, capturing the '\n","             'contextual information.\\n'\n","             '\\n'\n","             '2. Decoder: It generates the output sequence based on the '\n","             'encoded representations and attended context from the encoder.\\n'\n","             '\\n'\n","             '3. Attention mechanism: This is a crucial part of the '\n","             'Transformer. It computes the weighted sum of the input sequence '\n","             'elements, giving importance to specific words based on their '\n","             'relevance to the current context.\\n'\n","             '\\n'\n","             'Transformers have significantly advanced the field of NLP and '\n","             'have been used in numerous state-of-the-art models, such as '\n","             \"OpenAI's GPT, Google's BERT, and many other language models. \"\n","             'They enable models to understand and generate human-like '\n","             'language by capturing extensive context and relationships.',\n","  'role': 'assistant'},\n"," {'content': 'what are applications of transformers?', 'role': 'user'},\n"," {'content': 'Transformers have found extensive applications across various '\n","             'domains. Some of the notable applications of Transformers '\n","             'include:\\n'\n","             '\\n'\n","             '1. Machine Translation: Transformers have greatly improved '\n","             'machine translation. Models like Google\\'s \"Transformer\" and '\n","             'Facebook\\'s \"Fairseq\" have achieved high-quality translations by '\n","             'leveraging the attention mechanism to capture contextual '\n","             'information.\\n'\n","             '\\n'\n","             '2. Natural Language Processing (NLP): Transformers have been '\n","             'significant in advancing NLP tasks, such as text classification, '\n","             'sentiment analysis, named entity recognition, question '\n","             'answering, and summarization. Large-scale pre-trained '\n","             'transformer models like BERT (Bidirectional Encoder '\n","             'Representations from Transformers) and GPT (Generative '\n","             'Pre-trained Transformer) have achieved state-of-the-art '\n","             'performance in a wide range of NLP tasks.\\n'\n","             '\\n'\n","             '3. Speech Recognition: Transformers have been applied to speech '\n","             'recognition tasks, improving accuracy and performance. By '\n","             'considering the contextual dependencies in speech sequences, '\n","             'Transformers have shown promising results in transcribing audio '\n","             'to text.\\n'\n","             '\\n'\n","             '4. Image Recognition: Transformers have also been used in '\n","             'computer vision tasks, including image classification, object '\n","             'detection, and image generation. Vision Transformer (ViT) is an '\n","             'application example where transformers process image patches and '\n","             'outperform traditional convolutional neural networks (CNNs) in '\n","             'certain cases.\\n'\n","             '\\n'\n","             '5. Reinforcement Learning: Transformers have been combined with '\n","             'reinforcement learning algorithms to enable better '\n","             'decision-making in dynamic environments. These models can learn '\n","             'to perform complex tasks by interacting with an environment and '\n","             'receiving feedback.\\n'\n","             '\\n'\n","             '6. Recommendation Systems: Transformers have been applied to '\n","             'recommender systems, enhancing personalization and understanding '\n","             'user preferences. They can model sequential dependencies between '\n","             'user actions and capture complex patterns in user behavior.\\n'\n","             '\\n'\n","             '7. Generative Models: Transformers have been used to generate '\n","             'text and other content, including poetry, stories, and code. '\n","             'With their capacity to model long-range dependencies and '\n","             'contextual information, transformers can generate coherent and '\n","             'contextually relevant outputs.\\n'\n","             '\\n'\n","             'The applications of transformers are ever-expanding as '\n","             'researchers continue to explore and develop new architectures '\n","             'and techniques that leverage the power of transformers in '\n","             'various domains.',\n","  'role': 'assistant'},\n"," {'content': 'what is my name?', 'role': 'user'},\n"," {'content': 'Your name is Abhiprakash.', 'role': 'assistant'}]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-ad4cf2607ad9>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_chat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mmodel_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_chatgpt_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]},{"cell_type":"code","source":["\"\"\"\n","\n","Fine-tuning with gpt-3.5-turbo\n","\n","While gpt-3.5-turbo offers a cost-effective and efficient alternative to text-davinci-003 for most use cases,\n","\n","it's important to do fine-tuning base GPT-3 models. for all developers\n","\n","If you're unfamiliar with fine-tuning, it's a process of further training a pre-trained language model on a specific task or domain\n","to improve its performance on that task. With fine-tuning, developers can create custom models that are tailored to their specific use case.\n","\n","However, even without the option to fine-tune, gpt-3.5-turbo still provides powerful and cost-effective language generation capabilities.\n","\n","\"\"\""],"metadata":{"id":"sxtLJatEa8JM"},"execution_count":null,"outputs":[]}]}